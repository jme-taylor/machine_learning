{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c6bd0d2-6050-4a97-ad85-6623abc36e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ed773",
   "metadata": {},
   "source": [
    "# Classification Metrics\n",
    "\n",
    "Here, I'm running through various different classification metrics, and using scikit learn's `metrics` modules equivalents as a benchmark to make sure mine are running as expected.\n",
    "\n",
    "First, I'll load MNIST as a default classification problem, and use a `SGDClassifier` to get some baseline scores, then compare my home coded metrics against scikit learn's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe96684-3f6d-4ca5-8709-ad8e8382a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist['data'], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209fb5bc-237c-4fa6-97ee-e83ee25ec57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315e0af0-c6a5-4af7-ab13-f96f90ca1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making it a binary classification problem if required\n",
    "y_train_2 = (y_train == '2')\n",
    "y_test_2 = (y_test == '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54783555-e166-4f72-97a2-a3319d1be724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier()\n",
    "sgd_clf.fit(X_train, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89f39d3c-11e9-478a-a76d-d5455a6af9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_2 = sgd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecb39a",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is defined as the amount of correct predictions divided by thet total amount of predictions made. Below I check `scikit learn`'s version of this against my own to see how my implementation performs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58f6c9d0-3a1a-43f8-a17e-4659039b045b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9557"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test_2, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00a19b10-7713-41c3-aed3-8f4c793c53d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9557"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my version\n",
    "from machine_learning.metrics import accuracy\n",
    "\n",
    "accuracy(y_test_2, y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89c4d9a",
   "metadata": {},
   "source": [
    "It works, which is good. Accuracy is considered to be quite a flawed metric in evaluating classifiers. This is because it handles datasets where the target variable isn't evenly distributed poorly. Imagine a dataset with 99% of the samples having a target of `0` and 1% having `1`. You can make a 99% accurate classifier by predicting `0` for every single instance. A practical example of this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97e0c43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set = 0.0993\n",
      "Test set = 0.1032\n"
     ]
    }
   ],
   "source": [
    "# check balance of whole dataset classes\n",
    "print(f'Train set = {y_train_2.sum()/len(y_train_2)}')\n",
    "print(f'Test set = {y_test_2.sum()/len(y_test_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8d68d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an array of always false predictions\n",
    "y_pred_never_2 = np.zeros(len(y_test_2), dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "111f5666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8968"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate this with our accuracy metrics\n",
    "accuracy(y_test_2, y_pred_never_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50d7ce",
   "metadata": {},
   "source": [
    "As shown, you can score high accuracy with poor classifiers, so more nuanced metrics should be used for proper evaluation of a classifier.\n",
    "\n",
    "## Precision\n",
    "\n",
    "The precision of a classifier is defined as the amount of true positives divided by the sum of true positives and false positives. This can be thought of as the accuracy of the classifier's positive predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "281e97c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7196122296793438"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision_score(y_test_2, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b0f65a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7196122296793438"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from machine_learning.metrics import precision\n",
    "\n",
    "precision(y_test_2, y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1405dd61",
   "metadata": {},
   "source": [
    "So the implemention of precision looks like it's working.\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall is defined as the number of true positives divided by the sum of true positives and false negatives. In other words, it's the ratio of positive instances that are correctly identified by the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7056059b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935077519379845"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall_score(y_test_2, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc5b75d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.935077519379845"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from machine_learning.metrics import recall\n",
    "\n",
    "recall(y_test_2, y_pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e24063b",
   "metadata": {},
   "source": [
    "This implementation looks to be working too.\n",
    "\n",
    "### Precision Recall Curve\n",
    "\n",
    "All of the previous metrics were using a threshold for prediction 0.5. We can change this value, and observe the tradeoffs we're making in precision and recall as a result of this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
